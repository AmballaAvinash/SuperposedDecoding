{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35f1a95-7c8b-4b50-a184-8f53d9538476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.util import ngrams\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import GPT2Tokenizer ,  GPT2Model, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, LogitsProcessorList, T5ForConditionalGeneration, T5Tokenizer, MT5ForConditionalGeneration, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e54223-a754-43c0-8830-f3317e0362cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_data_set(split,dataset_name, dataset_subname):\n",
    "        data = {}\n",
    "        data[split] = datasets.load_dataset(dataset_name,dataset_subname, split=\"validation\",trust_remote_code=True, streaming=True)\n",
    "        return data[split]\n",
    "\n",
    "\n",
    "def ele_dist_k_from_idx(lst, start_index, k):\n",
    "    return lst[start_index::k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7637a557-59ba-48f3-9ab6-384d3325e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesize = 1000\n",
    "batch = 10\n",
    "random.seed(41)\n",
    "data = list(load_hf_data_set('validation','wmt19','de-en').take(samplesize))\n",
    "data = [x[\"translation\"] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e747237f-0b08-4d3b-a684-f3456ec65cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': 'München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern',\n",
       " 'en': 'Munich 1856: Four maps that will change your view of the city'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7c5c09-c61d-438e-9c92-820f72a260be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684e08c8-8d95-4edd-ac3d-840467a6ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_dhruveshpate_umass_edu/aamballa_umass_edu/.conda/envs/Superposed/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "563c41a3-3b52-47a5-a236-36b8115c4cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cls_tokenizer = AutoTokenizer\n",
    "cls_model = AutoModelForCausalLM\n",
    "tokenizer_args = {}\n",
    "device_map=\"auto\"\n",
    "torch_dtype=torch.float16\n",
    "load_in_8bit = False\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "\n",
    "if model_name==\"flan-t5\":\n",
    "    model = \"google/flan-t5-large\"\n",
    "    cls_model = T5ForConditionalGeneration\n",
    "    cls_tokenizer = T5Tokenizer\n",
    "\n",
    "elif model_name == \"m2m\":\n",
    "    model = \"facebook/m2m100_418M\"\n",
    "    cls_model = M2M100ForConditionalGeneration\n",
    "    cls_tokenizer = M2M100Tokenizer\n",
    "    \n",
    "elif model_name == \"mt0\":\n",
    "    model = \"bigscience/mt0-large\"\n",
    "    cls_model = AutoModelForSeq2SeqLM\n",
    "    cls_tokenizer = AutoTokenizer\n",
    "\n",
    "\n",
    "    \n",
    "tokenizer = cls_tokenizer.from_pretrained(model, **tokenizer_args)\n",
    "\n",
    "if  model_name == \"m2m\":\n",
    "    tokenizer.src_lang = \"fr\"\n",
    "\n",
    "if load_in_8bit:\n",
    "    # breakpoint()\n",
    "    bnb_config= BitsAndBytesConfig(load_in_8bit=True,)\n",
    "    model = cls_model.from_pretrained(model,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        device_map=device_map,\n",
    "                                        quantization_config=bnb_config,\n",
    "                                        # low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "                                        cache_dir = '/work/pi_dhruveshpate_umass_edu/aamballa_umass_edu/models/.cache',\n",
    "                                        trust_remote_code=True,\n",
    "                                        )\n",
    "\n",
    "else:\n",
    "    model = cls_model.from_pretrained(model,\n",
    "                                        torch_dtype=torch_dtype,\n",
    "                                        device_map=device_map,\n",
    "                                        # low_cpu_mem_usage=low_cpu_mem_usage,\n",
    "                                        cache_dir = '/work/pi_dhruveshpate_umass_edu/aamballa_umass_edu/models/.cache',\n",
    "                                        trust_remote_code=True,\n",
    "                                        load_in_8bit=load_in_8bit)\n",
    "\n",
    "# tokenizer.pad_token =  tokenizer.eos_token\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# model.eval()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1bd7b84-6952-49b9-86bf-56f5f3087a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serialize import serialize_tree, deserialize_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6471a09c-b18c-4608-b8b5-b1ca2ba0b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = torch.nn.LogSoftmax()\n",
    "def modelrun(model, input_ids, max_len, k):\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]   # (batch, seq len, vocab size)\n",
    "    \n",
    "        # select tok k \n",
    "        topk_probs, token_indices = torch.topk(next_token_logits,k, dim= -1)\n",
    "        log_topk_probs = log_softmax(topk_probs)\n",
    "\n",
    "        # stoing node, log probs for leaf nodes\n",
    "        out = []\n",
    "        if max_len == 1:\n",
    "            for i in range(k):\n",
    "                out.append((log_topk_probs[0][i].item(),token_indices[0][i].item(),1))\n",
    "            return out \n",
    "                \n",
    "        # preorder travesal such that top tokens are visited first\n",
    "        for i in range(k):\n",
    "            out.append((log_topk_probs[0][i].item(),token_indices[0][i].item(),0))\n",
    "            input1 = torch.cat([input_ids[0], torch.tensor([token_indices[0][i]], device = \"cuda\")], dim=-1).unsqueeze(0)\n",
    "            temp = modelrun(model,input1 , max_len-1, k)\n",
    "            for j in temp:\n",
    "                out.append(j)\n",
    "\n",
    "        return out \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c125497-2250-41d9-bcba-923006428a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   2%|███▍                                                                                                                                                       | 22/1000 [02:24<1:47:33,  6.60s/it]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Predicting:  17%|██████████████████████████▏                                                                                                                               | 170/1000 [18:42<1:30:59,  6.58s/it]"
     ]
    }
   ],
   "source": [
    "default_fwd_instruction = \"Translate the following German sentence to an English sentence.\"\n",
    "default_fwd_input_prefix = \"German sentence: \"\n",
    "default_fwd_target_prefix = \". English sentence: \"\n",
    "\n",
    "for idx, d in enumerate(tqdm(data, desc=\"Predicting\")):\n",
    "\n",
    "    prompt_arr = [default_fwd_instruction,default_fwd_input_prefix]\n",
    "    prompt_arr.append(d['de'])\n",
    "    prompt_arr.append(default_fwd_target_prefix)\n",
    "    input_prompt = (' ').join(prompt_arr)  # join the sentences\n",
    "    \n",
    "    input_ids = tokenizer(input_prompt, return_tensors='pt').input_ids.to('cuda')\n",
    "    \n",
    "    max_len = 5\n",
    "    k = 5 \n",
    "    \n",
    "    nodes = modelrun(model, input_ids, max_len, k)\n",
    "    #  adding the prefix to the tree\n",
    "    nodes.insert(0, (1,1,0))\n",
    "\n",
    "    # serialize the tree\n",
    "    serialize_tree(k, max_len, nodes, f\"data/sample{idx}.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e7d98-95a4-45ed-a223-45022901c4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e34092dc-d468-445c-9b17-edd580ce66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "47d5190a-8d91-4153-a387-2b3471f074a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1, 0),\n",
       " (-0.53369140625, 11, 0),\n",
       " (-0.67529296875, 475, 0),\n",
       " (-0.1954345703125, 314, 0),\n",
       " (-0.53125, 1101, 0),\n",
       " (-0.33251953125, 407, 1),\n",
       " (-1.2626953125, 635, 1),\n",
       " (-0.88623046875, 836, 0),\n",
       " (-0.0001493692398071289, 470, 1),\n",
       " (-8.8125, 6, 1),\n",
       " (-1.728515625, 340, 0),\n",
       " (-0.1771240234375, 338, 0),\n",
       " (-0.424560546875, 407, 1),\n",
       " (-1.0615234375, 1327, 1),\n",
       " (-1.8173828125, 318, 0),\n",
       " (-0.6884765625, 407, 1),\n",
       " (-0.69775390625, 257, 1),\n",
       " (-0.71142578125, 290, 0),\n",
       " (-0.1597900390625, 314, 0),\n",
       " (-0.53955078125, 1842, 0),\n",
       " (-0.346435546875, 284, 1),\n",
       " (-1.228515625, 262, 1),\n",
       " (-0.875, 1101, 0),\n",
       " (-0.57373046875, 1464, 1),\n",
       " (-0.82861328125, 1654, 1),\n",
       " (-1.9130859375, 356, 0),\n",
       " (-0.55419921875, 423, 0),\n",
       " (-0.265869140625, 257, 1),\n",
       " (-1.4541015625, 587, 1),\n",
       " (-0.8544921875, 821, 0),\n",
       " (-0.5908203125, 1464, 1),\n",
       " (-0.80712890625, 1111, 1),\n",
       " (-0.8828125, 13, 0),\n",
       " (-0.44140625, 314, 0),\n",
       " (-0.46728515625, 1842, 0),\n",
       " (-0.338623046875, 284, 0),\n",
       " (-0.250732421875, 711, 1),\n",
       " (-1.5068359375, 2342, 1),\n",
       " (-1.2470703125, 262, 0),\n",
       " (-0.64599609375, 1109, 1),\n",
       " (-0.74267578125, 835, 1),\n",
       " (-0.9853515625, 588, 0),\n",
       " (-0.11798095703125, 284, 0),\n",
       " (-0.340576171875, 711, 1),\n",
       " (-1.2421875, 423, 1),\n",
       " (-2.1953125, 262, 0),\n",
       " (-0.60009765625, 1109, 1),\n",
       " (-0.7958984375, 835, 1),\n",
       " (-1.0302734375, 679, 0),\n",
       " (-0.66259765625, 338, 0),\n",
       " (-0.357421875, 257, 0),\n",
       " (-0.57177734375, 1049, 1),\n",
       " (-0.83154296875, 922, 1),\n",
       " (-1.2021484375, 523, 0),\n",
       " (-0.383544921875, 13779, 1),\n",
       " (-1.1435546875, 8030, 1),\n",
       " (-0.724609375, 318, 0),\n",
       " (-0.5009765625, 257, 0),\n",
       " (-0.58984375, 1049, 1),\n",
       " (-0.80859375, 845, 1),\n",
       " (-0.931640625, 845, 0),\n",
       " (-0.273193359375, 8030, 1),\n",
       " (-1.4306640625, 34264, 1)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    deserialize_tree(k, max_len, f\"data/sample{i}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8dc4d-c641-4627-bcb7-ab1233737fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a18d99-d6cd-4ee0-8f7b-a03ebc09df95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8842b7a-57d9-4e20-aafd-a580e4694121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f838541-df45-4cf1-b345-cb1455ac23b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
